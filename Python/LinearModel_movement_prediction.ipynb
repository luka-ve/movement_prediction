{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "psychological-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import math\n",
    "import h5py\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "worthy-interaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"D:/Coding/Thesis/Data/STFT Output/**/*.h5\"\n",
    "data_files = glob.glob(data_path, recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-spare",
   "metadata": {},
   "source": [
    "# General data config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "vulnerable-bruce",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {}\n",
    "\n",
    "config['model_folder'] = 'D:/Coding/Thesis/movement_prediction/Models/'\n",
    "\n",
    "config['EEG_window_length_in_ms'] = 2000\n",
    "\n",
    "config['delta_time_k'] = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "config['tap_count_times_p'] = np.array([0.5, 1, 5, 10, 50, 100, 500])\n",
    "\n",
    "config['n_output_features'] = np.sum([config['delta_time_k'].shape, config['tap_count_times_p'].shape])\n",
    "\n",
    "config['read_config_from_h5'] = True\n",
    "\n",
    "config['EEG_sampling_rate'] = 1000\n",
    "config['stft_stride'] = 128\n",
    "config['sampling_rate_after_stft'] = config['EEG_sampling_rate'] / config['stft_stride']\n",
    "config['sample_length_after_stft'] = 1000 / config['sampling_rate_after_stft']\n",
    "\n",
    "config['tap_count_times_in_samples'] = np.multiply(config['tap_count_times_p'], config['EEG_sampling_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-basket",
   "metadata": {},
   "source": [
    "# Load entire participant's data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "waiting-married",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "Number of activity windows: 7\n",
      "[(64, 12300, 11), (64, 17318, 11), (64, 941, 11), (64, 802, 11), (64, 649, 11), (64, 649, 11), (64, 7450, 11)]\n",
      "[(1, 698), (1, 876), (1, 50), (1, 54), (1, 4), (1, 3), (1, 516)]\n"
     ]
    }
   ],
   "source": [
    "def load_data(h5_files):\n",
    "    EEG, taps = [], []\n",
    "    \n",
    "    # Makes sure that if a single file is passed in, it is put into a list\n",
    "    if type(h5_files) != 'list':\n",
    "        h5_files = [h5_files]\n",
    "    \n",
    "    for f in h5_files:\n",
    "        with h5py.File(f, 'r') as f_open:\n",
    "            if config['read_config_from_h5']:\n",
    "                    config['EEG_sampling_rate'] = f_open.attrs['original_sampling_rate']\n",
    "                    config['stft_stride'] = f_open.attrs['stft_hopsize']\n",
    "            \n",
    "            for session in list(f_open.keys()):\n",
    "                for activity_window in list(f_open[session].keys()):\n",
    "                    if activity_window.startswith('window_'):\n",
    "                        EEG.append(np.array(f_open[session][activity_window]['stft'], dtype='float32'))\n",
    "                        taps.append(np.array(f_open[session][activity_window]['taps']))\n",
    "    \n",
    "    ############# Inf found in data!\n",
    "    for i in EEG:\n",
    "        i[np.isinf(i)] = 0\n",
    "    \n",
    "    return (EEG, taps)\n",
    "\n",
    "stft, taps_data = load_data('D:\\\\Coding\\\\Thesis\\\\Data\\\\STFT Output\\\\DS99.h5')\n",
    "\n",
    "\n",
    "# Add dynamic config parameter\n",
    "config['window_length_sftf_samples'] = np.ceil(config['EEG_window_length_in_ms'] / 1000 * config['EEG_sampling_rate'] / config['stft_stride']).round().astype(np.int16)[0]\n",
    "\n",
    "print(config['window_length_sftf_samples'])\n",
    "print(f'Number of activity windows: {len(stft)}')\n",
    "print([x.shape for x in stft])\n",
    "print([y.shape for y in taps_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-alexandria",
   "metadata": {},
   "source": [
    "# Normalize input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "grave-panel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN: 0\n",
      "Inf: 0\n"
     ]
    }
   ],
   "source": [
    "def normalize_input(_input, normalize_over_eeg_channels=False):\n",
    "    '''stft: A list of numpy arrays.\n",
    "    \n",
    "   Normalize over all features, ignoring  \n",
    "    \n",
    "    '''\n",
    "    stft_cat = np.concatenate(_input, axis=1)\n",
    "    \n",
    "    print(f'NaN: {np.sum(np.isnan(stft_cat))}')\n",
    "    print(f'Inf: {np.sum(np.isinf(stft_cat))}')\n",
    "    \n",
    "    if normalize_over_eeg_channels:\n",
    "        axes = (0, 1)\n",
    "    else:\n",
    "        axes = 1\n",
    "    \n",
    "    means = np.mean(stft_cat, axis=axes, keepdims=True)\n",
    "    st_devs = np.std(stft_cat, axis=axes, keepdims=True)\n",
    "    \n",
    "    out = [(i - means) - st_devs for i in _input]\n",
    "    \n",
    "    return out\n",
    "    \n",
    "\n",
    "stft_norm = normalize_input(stft)\n",
    "\n",
    "#print([x.shape[1] for x in stft_norm])\n",
    "\n",
    "# Remove end of EEG activity, since there is no activity of interest\n",
    "\n",
    "seconds_to_remove = 28\n",
    "samples_to_remove = np.ceil(config['sampling_rate_after_stft'] * seconds_to_remove).astype(np.int)\n",
    "\n",
    "stft_norm_data = [x[:, :-samples_to_remove, :] for x in stft_norm]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-blank",
   "metadata": {},
   "source": [
    "# Split data into train, validation, and test set\n",
    "\n",
    "For every window where taps were recorded, we use the first 80% of activity as test set. We then split the remaining 20% in half to assign to validation and test set, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hired-galaxy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total taps:\t [698, 876, 50, 54, 4, 3, 516]\n",
      "Train set:\t [(545,), (656,), (48,), (35,), (373,)]\n",
      "Valid set:\t [(63,), (111,), (0,), (5,), (68,)]\n",
      "Test set:\t [(90,), (109,), (2,), (14,), (75,)]\n"
     ]
    }
   ],
   "source": [
    "stft_norm = {}\n",
    "stft_norm['train'] = []\n",
    "stft_norm['validation'] = []\n",
    "stft_norm['test'] = []\n",
    "\n",
    "taps = {'train': [], 'validation': [], 'test': []}\n",
    "\n",
    "for i, x in enumerate(stft_norm_data):\n",
    "    x_len = x.shape[1]\n",
    "    \n",
    "    # Check if activity window length is too short.\n",
    "    # If so, add it to train set.\n",
    "    # If extremely short, discard window\n",
    "    if taps_data[i].shape[1] < 5:\n",
    "        continue\n",
    "    \n",
    "    train_idx = np.arange(0, (x_len//10 * 8))\n",
    "\n",
    "    \n",
    "    validation_idx = np.arange((x_len//10 * 8), (x_len//10 * 9))\n",
    "    test_idx = np.arange((x_len//10 * 9), x_len)\n",
    "    \n",
    "    stft_norm['train'].append(x[:, train_idx, :])\n",
    "    stft_norm['validation'].append(x[:, validation_idx, :])\n",
    "    stft_norm['test'].append(x[:, test_idx, :])\n",
    "    \n",
    "    # Add taps\n",
    "    _taps = taps_data[i] / config['stft_stride']\n",
    "\n",
    "    _taps_train = _taps[np.logical_and(_taps > (train_idx[0]), \\\n",
    "                                       _taps < (train_idx[-1]))]\n",
    "    \n",
    "    _taps_vali = _taps[np.logical_and(_taps > (validation_idx[0]), \\\n",
    "                                      _taps < (validation_idx[-1]))]\n",
    "    \n",
    "    _taps_test = _taps[np.logical_and(_taps > (test_idx[0]), \\\n",
    "                                      _taps < (test_idx[-1]))]\n",
    "    \n",
    "    # Adjust tap timings to zero at beginning of sequence\n",
    "    _taps_train = _taps_train - (train_idx[0])\n",
    "    _taps_vali = _taps_vali - (validation_idx[0])\n",
    "    _taps_test = _taps_test - (test_idx[0])\n",
    "    \n",
    "    taps['train'].append(np.array(_taps_train, dtype=np.int))\n",
    "    taps['validation'].append(np.array(_taps_vali, dtype=np.int))\n",
    "    taps['test'].append(np.array(_taps_test, dtype=np.int))\n",
    "    \n",
    "print(f'Total taps:\\t {[y.shape[1] for y in taps_data]}')\n",
    "print(f'Train set:\\t {[y.shape for y in taps[\"train\"]]}')\n",
    "print(f'Valid set:\\t {[y.shape for y in taps[\"validation\"]]}')\n",
    "print(f'Test set:\\t {[y.shape for y in taps[\"test\"]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-accounting",
   "metadata": {},
   "source": [
    "# Generating a trainable data set\n",
    "\n",
    "Now, that we have split our data set into train, validation, and test set, we need to transform the timeseries data into a matrix of size $M \\times N$ with $M = \\text{number of samples}$ and $N = |\\text{features}| \\times \\text{window length}$.\n",
    "\n",
    "Each row is the input data shifted over by one sample. Columns is the entire input data for the window, but flattened into a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "spatial-lawsuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _set in stft_norm.items():\n",
    "    current_set = _set[1]\n",
    "\n",
    "    for i, row in enumerate(current_set):\n",
    "        current_set[i] = np.transpose(row, (1, 0, 2))\n",
    "        current_set[i] = np.reshape(current_set[i], (current_set[i].shape[0], -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proud-graphics",
   "metadata": {},
   "source": [
    "## Tap data generation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "resident-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taps_in_window(taps, window_end):      \n",
    "    tap_deltas = get_delta_taps(taps, window_end)\n",
    "\n",
    "    future_tap_n = get_n_future_taps(taps, window_end)\n",
    "\n",
    "    result = np.concatenate((tap_deltas, future_tap_n))\n",
    "\n",
    "    # Convert values to log to compress values\n",
    "    result = [np.log10(tap) if tap > 0 else tap for tap in result]\n",
    "\n",
    "    threshold_ms = 1\n",
    "    result = np.maximum(result, np.log10(threshold_ms))\n",
    "\n",
    "    return result\n",
    "    \n",
    "    \n",
    "def get_delta_taps(taps, window_idx):\n",
    "    n_k = len(config['delta_time_k'])\n",
    "\n",
    "    next_kth_taps = taps[taps > window_idx][:n_k]\n",
    "\n",
    "    # Ensure that if not enough taps were found the array is padded with 0s.\n",
    "    # This only occurs at the end of a window\n",
    "    if len(next_kth_taps) < n_k:\n",
    "        next_kth_taps = np.concatenate((next_kth_taps, np.zeros(n_k - len(next_kth_taps))))\n",
    "\n",
    "    tap_deltas = next_kth_taps - window_idx\n",
    "\n",
    "    return tap_deltas * config['stft_stride']\n",
    "\n",
    "\n",
    "def get_n_future_taps(taps, window_idx):\n",
    "    n_future_taps = np.zeros(len(config['tap_count_times_in_samples']))\n",
    "\n",
    "    for p_idx, p in enumerate(config['tap_count_times_in_samples']):\n",
    "        n_future_taps[p_idx] = len(\n",
    "            taps[\n",
    "                (taps > window_idx) &\n",
    "                (taps <= (window_idx + p))\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return n_future_taps\n",
    "\n",
    "def generate_trainable_data(X: dict, Y: dict, key: str, config: dict, stride=1, verbose=False):\n",
    "    '''\n",
    "    Returns the trainable data in matrix form.\n",
    "    \n",
    "    X and Y are dictionaries holding the raw training, validation, and test data.\n",
    "    key: A string specifying the key to access in X and Y\n",
    "    stride: If stride is n > 1, every n-th sample is taken. Higher stride values will result in less memory usage, but also less accurate training/predictions.\n",
    "    '''\n",
    "    win_length = config['window_length_sftf_samples']\n",
    "\n",
    "    window_lengths = [x.shape[0] // stride for x in X[key]]\n",
    "\n",
    "    data_set_X = None # Clear memory before reassignment\n",
    "    data_set_X = np.zeros([np.sum(window_lengths), X[key][0].shape[1] * win_length], dtype=np.float32)\n",
    "    data_set_Y = np.zeros([data_set_X.shape[0], config['n_output_features']], dtype=np.uint)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'Training X array has shape: {data_set_X.shape}')\n",
    "        print(f'X data array uses {data_set_X.nbytes // 1024 // 1024} MiB, {data_set_X.itemsize} bytes per item')\n",
    "        print(f'Training Y array has shape: {data_set_Y.shape}')\n",
    "        print(f'Y data array uses {data_set_Y.nbytes // 1024} KiB, {data_set_Y.itemsize} bytes per item')\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    for win_idx, stft_win in enumerate(X[key]):\n",
    "        for sample_idx in range(win_length, stft_win.shape[0], stride):\n",
    "            data_set_X[i, :] = stft_win[(sample_idx - win_length):sample_idx, :].flatten()\n",
    "            data_set_Y[i, :] = get_taps_in_window(taps[key][win_idx], sample_idx)\n",
    "\n",
    "            i += 1\n",
    "            \n",
    "    return data_set_X, data_set_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "substantial-category",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training X array has shape: (30160, 11264)\n",
      "X data array uses 1295 MiB, 4 bytes per item\n",
      "Training Y array has shape: (30160, 17)\n",
      "Y data array uses 2002 KiB, 4 bytes per item\n"
     ]
    }
   ],
   "source": [
    "train_stride = 1\n",
    "\n",
    "train_set_X, train_set_Y = None, None\n",
    "train_set_X, train_set_Y = generate_trainable_data(stft_norm, taps, 'train', config, stride=train_stride, verbose=True)\n",
    "\n",
    "# Converting to Fortran-contiguous array improves speed when fitting model\n",
    "train_set_X = np.asfortranarray(train_set_X, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verified-saying",
   "metadata": {},
   "source": [
    "# Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-payday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For now, let's save models in a list\n",
    "lm = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-contemporary",
   "metadata": {},
   "source": [
    "! Should replace linear_model.Lasso with linear_mode.LassoCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-directive",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for feature in range(train_set_Y.shape[1]):\n",
    "for feature in range(1): # Only use range(1) for debugging    \n",
    "    start = time.time()\n",
    "    print(f'Training feature {feature} ... ')\n",
    "    \n",
    "    # Fit model\n",
    "    lm.append(linear_model.Lasso(n_alphas=1, copy_X=False, cv=1, n_jobs=3, verbose=True))\n",
    "    lm[-1].fit(train_set_X, train_set_Y[:, feature])\n",
    "    \n",
    "    time_elapsed = time.time() - start\n",
    "    print(f'Done in {time_elapsed:.2f} seconds.')\n",
    "    \n",
    "train_set_X, train_set_Y = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "future-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_stride = 1\n",
    "\n",
    "test_set_X, test_set_Y = None, None\n",
    "test_set_X, test_set_Y = generate_trainable_data(stft_norm, taps, 'test', config, stride=test_stride, verbose=False)\n",
    "prediction = lm[0].predict(test_set_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "incredible-hawaii",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.886\n"
     ]
    }
   ],
   "source": [
    "RMSE = mean_squared_error(test_set_Y[:, 0], prediction, squared=False)\n",
    "print(f'RMSE: {RMSE:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-surgery",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New plots for new test framework\n",
    "%matplotlib inline\n",
    "\n",
    "#RMSE = np.sqrt(((_Y_hat[0] - _Y[0]) ** 2).mean(0))\n",
    "\n",
    "\n",
    "#fig, ((deltas_plt, n_taps_plt), (RMSE_deltas_plt, RMSE_ntaps_plt)) = plt.subplots(2, 2, figsize=(15, 15))\n",
    "\n",
    "plt.figure()\n",
    "fig, axs = plt.subplots(len(_Y), 1, figsize=(15, 5 * len(_Y)))\n",
    "\n",
    "\n",
    "for sequence_id in range(len(_Y)):\n",
    "    axs[sequence_id].plot(_Y_hat[sequence_id][:, output_feature], '--')\n",
    "    axs[sequence_id].plot(_Y[sequence_id][:, output_feature], '-')\n",
    "    #axs[sequence_id].plot(_Y_random[:, output_feature], '-')\n",
    "    axs[sequence_id].set_xlabel('Time in samples')\n",
    "    axs[sequence_id].set_ylabel('$\\Delta t$ in ms')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
